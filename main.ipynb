{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models, utils\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import os\n",
    "# # import pandas as pd\n",
    "\n",
    "# # def load_dataset_as_dataframe(subdir):\n",
    "# #     # Define the path to the dataset\n",
    "# #     data_dir = f'e:\\\\Bahria University\\\\Open-CV lab\\\\Project\\\\{subdir}'\n",
    "# #     print(f'Load dataset from `{subdir}` subdirectory')\n",
    "\n",
    "# #     # Create lists to store image paths and labels\n",
    "# #     image_paths = []\n",
    "# #     labels = []\n",
    "\n",
    "# #     # Get the list of class directories\n",
    "# #     classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "# #     classes.sort()\n",
    "# #     print('Classes:', classes)\n",
    "\n",
    "# #     # Map class names to labels\n",
    "# #     class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "\n",
    "# #     # Loop over each class directory\n",
    "# #     for class_name in classes:\n",
    "# #         class_dir = os.path.join(data_dir, class_name)\n",
    "# #         label = class_to_idx[class_name]\n",
    "# #         # Get all image files in the class directory\n",
    "# #         for img_name in os.listdir(class_dir):\n",
    "# #             img_path = os.path.join(class_dir, img_name)\n",
    "# #             if os.path.isfile(img_path):\n",
    "# #                 image_paths.append(img_path)\n",
    "# #                 labels.append(label)\n",
    "\n",
    "# #     # Create a DataFrame\n",
    "# #     data = pd.DataFrame({\n",
    "# #         'image_path': image_paths,\n",
    "# #         'label': labels\n",
    "# #     })\n",
    "\n",
    "# #     # Show the distribution of labels in the dataset\n",
    "# #     dataset_distribution_dict = {i: len(data[data['label'] == i]) for i in range(len(classes))}\n",
    "# #     print(dataset_distribution_dict)\n",
    "# #     print()\n",
    "\n",
    "# #     return data\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def load_dataset_as_dataframe(subdir, max_images=100):\n",
    "#     # Define the path to the dataset\n",
    "#     data_dir = f'e:\\\\Bahria University\\\\Open-CV lab\\\\Project\\\\{subdir}'\n",
    "#     print(f'Load dataset from `{subdir}` subdirectory')\n",
    "\n",
    "#     # Create lists to store image paths and labels\n",
    "#     image_paths = []\n",
    "#     labels = []\n",
    "\n",
    "#     # Get the list of class directories\n",
    "#     classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "#     classes.sort()\n",
    "#     print('Classes:', classes)\n",
    "\n",
    "#     # Map class names to labels\n",
    "#     class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "\n",
    "#     # Counter to track the number of images processed\n",
    "#     count = 0\n",
    "\n",
    "#     # Loop over each class directory\n",
    "#     for class_name in classes:\n",
    "#         class_dir = os.path.join(data_dir, class_name)\n",
    "#         label = class_to_idx[class_name]\n",
    "#         # Get all image files in the class directory\n",
    "#         for img_name in os.listdir(class_dir):\n",
    "#             if count >= max_images:\n",
    "#                 break  # Stop if the maximum number of images is reached\n",
    "#             img_path = os.path.join(class_dir, img_name)\n",
    "#             if os.path.isfile(img_path):\n",
    "#                 image_paths.append(img_path)\n",
    "#                 labels.append(label)\n",
    "#                 count += 1\n",
    "#         if count >= max_images:\n",
    "#             break  # Stop processing if the maximum number of images is reached\n",
    "\n",
    "#     # Create a DataFrame\n",
    "#     data = pd.DataFrame({\n",
    "#         'image_path': image_paths,\n",
    "#         'label': labels\n",
    "#     })\n",
    "\n",
    "#     # Show the distribution of labels in the dataset\n",
    "#     dataset_distribution_dict = {i: len(data[data['label'] == i]) for i in range(len(classes))}\n",
    "#     print(dataset_distribution_dict)\n",
    "#     print()\n",
    "\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_as_dataframe(subdir, max_images=100):\n",
    "    # Define the path to the dataset\n",
    "    data_dir = f'e:\\\\Bahria University\\\\Open-CV lab\\\\Project\\\\{subdir}'\n",
    "    print(f'Loading dataset from `{subdir}` subdirectory')\n",
    "\n",
    "    # Create lists to store image paths and labels\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Get the list of class directories\n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    classes.sort()\n",
    "    print('Classes:', classes)\n",
    "\n",
    "    # Map class names to labels\n",
    "    class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "\n",
    "    # Counter to track the number of images processed\n",
    "    count = 0\n",
    "\n",
    "    # Loop over each class directory\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        label = class_to_idx[class_name]\n",
    "        # Get all image files in the class directory\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if count >= max_images:\n",
    "                break  # Stop if the maximum number of images is reached\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if os.path.isfile(img_path):\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(label)\n",
    "                count += 1\n",
    "        if count >= max_images:\n",
    "            break  # Stop processing if the maximum number of images is reached\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    # Show the distribution of labels in the dataset\n",
    "    dataset_distribution_dict = {i: len(data[data['label'] == i]) for i in range(len(classes))}\n",
    "    print(dataset_distribution_dict)\n",
    "    print()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset from `train` subdirectory\n",
      "Classes: ['0', '1', '2', '3', '4']\n",
      "{0: 100, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "\n",
      "Load dataset from `test` subdirectory\n",
      "Classes: ['0', '1', '2', '3', '4']\n",
      "{0: 100, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define classes\n",
    "classes = ['0', '1', '2', '3', '4']\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_df = load_dataset_as_dataframe('train')\n",
    "test_df = load_dataset_as_dataframe('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations for training and testing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet standards\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)  # Reset the index for consistency\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'image_path']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances for training and testing\n",
    "train_dataset = KneeDataset(train_df, transform=train_transform)\n",
    "test_dataset = KneeDataset(test_df, transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 27844, 13880) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Muhammad Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Muhammad Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Get a batch of training data\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Make a grid from batch\u001b[39;00m\n\u001b[0;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmake_grid(images)\n",
      "File \u001b[1;32mc:\\Users\\Muhammad Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Muhammad Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muhammad Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Muhammad Talha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 27844, 13880) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Function to display images\n",
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    # Unnormalize\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img  = std * img + mean\n",
    "    img  = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a batch of training data\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = utils.make_grid(images)\n",
    "\n",
    "# Display images\n",
    "plt.figure(figsize=(10, 10))\n",
    "imshow(out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
